Menu

## Memory Indexing

Relevant source files
- [.npmrc](https://github.com/openclaw/openclaw/blob/bf6ec64f/.npmrc)
- [.prettierignore](https://github.com/openclaw/openclaw/blob/bf6ec64f/.prettierignore)
- [CHANGELOG.md](https://github.com/openclaw/openclaw/blob/bf6ec64f/CHANGELOG.md)
- [README.md](https://github.com/openclaw/openclaw/blob/bf6ec64f/README.md)
- [assets/avatar-placeholder.svg](https://github.com/openclaw/openclaw/blob/bf6ec64f/assets/avatar-placeholder.svg)
- [docs/cli/memory.md](https://github.com/openclaw/openclaw/blob/bf6ec64f/docs/cli/memory.md)
- [docs/concepts/memory.md](https://github.com/openclaw/openclaw/blob/bf6ec64f/docs/concepts/memory.md)
- [extensions/memory-core/package.json](https://github.com/openclaw/openclaw/blob/bf6ec64f/extensions/memory-core/package.json)
- [package.json](https://github.com/openclaw/openclaw/blob/bf6ec64f/package.json)
- [pnpm-lock.yaml](https://github.com/openclaw/openclaw/blob/bf6ec64f/pnpm-lock.yaml)
- [pnpm-workspace.yaml](https://github.com/openclaw/openclaw/blob/bf6ec64f/pnpm-workspace.yaml)
- [scripts/clawtributors-map.json](https://github.com/openclaw/openclaw/blob/bf6ec64f/scripts/clawtributors-map.json)
- [scripts/update-clawtributors.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/scripts/update-clawtributors.ts)
- [scripts/update-clawtributors.types.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/scripts/update-clawtributors.types.ts)
- [src/agents/memory-search.test.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/agents/memory-search.test.ts)
- [src/agents/memory-search.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/agents/memory-search.ts)
- [src/agents/pi-embedded-runner-extraparams.live.test.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/agents/pi-embedded-runner-extraparams.live.test.ts)
- [src/agents/pi-embedded-runner-extraparams.test.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/agents/pi-embedded-runner-extraparams.test.ts)
- [src/agents/tools/memory-tool.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/agents/tools/memory-tool.ts)
- [src/auto-reply/reply/agent-runner.heartbeat-typing.runreplyagent-typing-heartbeat.retries-after-compaction-failure-by-resetting-session.test.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/auto-reply/reply/agent-runner.heartbeat-typing.runreplyagent-typing-heartbeat.retries-after-compaction-failure-by-resetting-session.test.ts)
- [src/cli/memory-cli.test.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/cli/memory-cli.test.ts)
- [src/cli/memory-cli.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/cli/memory-cli.ts)
- [src/config/schema.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/config/schema.ts)
- [src/config/types.tools.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/config/types.tools.ts)
- [src/config/zod-schema.agent-runtime.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/config/zod-schema.agent-runtime.ts)
- [src/index.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/index.ts)
- [src/memory/index.test.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/index.test.ts)
- [src/memory/internal.test.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/internal.test.ts)
- [src/memory/internal.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/internal.ts)
- [src/memory/manager-cache-key.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager-cache-key.ts)
- [src/memory/manager.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts)
- [src/memory/sync-memory-files.ts](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/sync-memory-files.ts)
- [ui/package.json](https://github.com/openclaw/openclaw/blob/bf6ec64f/ui/package.json)
- [ui/src/styles.css](https://github.com/openclaw/openclaw/blob/bf6ec64f/ui/src/styles.css)
- [ui/src/styles/layout.mobile.css](https://github.com/openclaw/openclaw/blob/bf6ec64f/ui/src/styles/layout.mobile.css)

This document explains how OpenClaw indexes workspace memory files for semantic search. Memory indexing transforms Markdown files into searchable embeddings stored in SQLite, enabling vector similarity search and hybrid BM25+vector retrieval.

For information about memory file layout and usage patterns, see [Memory](https://deepwiki.com/openclaw/openclaw/7-memory-system). For configuring memory search settings, see [Memory Configuration](https://deepwiki.com/openclaw/openclaw/7.1-memory-configuration). For search mechanics and query execution, see [Memory Search](https://deepwiki.com/openclaw/openclaw/7.3-memory-search).

---

## Overview

Memory indexing is the process of discovering workspace files, splitting them into chunks, generating embeddings, and storing them in a searchable index. The system supports:

- **File Discovery**: Recursively scans `MEMORY.md`, `memory/*.md`, session transcripts, and additional configured paths
- **Chunking**: Splits files into token-sized segments with configurable overlap
- **Embedding Generation**: Uses OpenAI, Gemini, or local models to create vector representations
- **Batch Processing**: Leverages batch APIs (OpenAI/Gemini) for efficient bulk embedding generation
- **Incremental Sync**: Tracks file hashes to avoid reindexing unchanged content
- **Caching**: Stores embeddings in SQLite to prevent redundant API calls
- **Hybrid Storage**: Combines vector search (sqlite-vec) with full-text search (FTS5) for optimal retrieval

Sources: [src/memory/manager.ts 1-1000](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L1-L1000) [docs/concepts/memory.md 1-174](https://github.com/openclaw/openclaw/blob/bf6ec64f/docs/concepts/memory.md#L1-L174)

---

## Index Manager Lifecycle

```
ChangedUnchangedMemoryIndexManager.get()Constructor:
- Open Database
- Load Extension
- Ensure Schema
- Start Watcherssync() calledHash Comparison:
buildFileEntry()Chunk Files:
chunkMarkdown()Generate EmbeddingsStore in SQLiteclose()
```

**Index Manager Lifecycle**

The `MemoryIndexManager` singleton manages the entire indexing pipeline. On initialization, it opens the SQLite database, loads the sqlite-vec extension if available, ensures schema exists, and starts file watchers if configured. The `sync()` method orchestrates file discovery, change detection, chunking, embedding generation, and storage.

Sources: [src/memory/manager.ts 119-250](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L119-L250) [src/memory/manager.ts 176-206](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L176-L206)

---

## File Discovery and Hashing

### Discovery Process

```
listMemoryFiles()Check MEMORY.md
Check memory.mdScan memory/ directory
(recursive *.md)Process extraPaths
(dirs & files)Skip symlinksReturn absolute paths
```

**File Discovery Flow**

File discovery happens in three phases:

1. Check for primary memory files (`MEMORY.md` or `memory.md` in workspace root)
2. Recursively scan the `memory/` directory for all `.md` files
3. Process any additional paths from `memorySearch.extraPaths` configuration

Symlinks are always skipped to prevent loops and unexpected behavior.

Sources: [src/memory/internal.ts 93-129](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/internal.ts#L93-L129) [src/memory/manager.ts 771-806](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L771-L806)

### File Entry Structure

```
"Stored in DB"MemoryFileEntry+string path+string absPath+number mtimeMs+number size+string hashFileRecord+string path+string hash+number mtimeMs+number size+string provider+string model+number indexed_at
```

**File Entry and Database Record**

Each discovered file is converted to a `MemoryFileEntry` containing:

- Relative path (workspace-relative)
- Absolute path (filesystem location)
- Modification time in milliseconds
- File size in bytes
- Hash (SHA-256 of content)

The hash enables incremental indexing: files are only reprocessed when their hash changes. The `files` table in SQLite tracks indexed state per provider/model combination.

Sources: [src/memory/internal.ts 6-12](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/internal.ts#L6-L12) [src/memory/manager.ts 771-830](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L771-L830)

---

## Chunking Strategy

### Token-Based Chunking

The chunking algorithm splits Markdown files into overlapping segments optimized for embedding models:

| Parameter | Default | Purpose |
| --- | --- | --- |
| `chunking.tokens` | 400 | Target tokens per chunk |
| `chunking.overlap` | 80 | Overlap tokens between chunks |
| Line-based | Yes | Split on line boundaries |
| Preserve context | Yes | Include headers and context |

**Chunking Configuration**

Chunks are created by:

1. Reading file content line-by-line
2. Estimating tokens per line (1 token ≈ 4 characters)
3. Accumulating lines until token limit reached
4. Creating overlap by including trailing lines from previous chunk

Sources: [src/memory/internal.ts 141-209](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/internal.ts#L141-L209) [src/agents/memory-search.ts 76-77](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/agents/memory-search.ts#L76-L77)

### Chunk Structure

```
"Stored as"MemoryChunk+number startLine+number endLine+string text+string hashChunkRecord+string id+string file_path+number start_line+number end_line+string text+string hash+string provider+string model+string source+number indexed_at
```

**Chunk Data Model**

Each chunk tracks:

- Start/end line numbers for snippet extraction
- Full text content for embedding
- Hash for deduplication
- Source type (`"memory"` or `"sessions"`)

The `chunks` table stores all chunks with references back to their source file. Chunk hashes prevent duplicate storage when files overlap or are reorganized.

Sources: [src/memory/internal.ts 14-19](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/internal.ts#L14-L19) [src/memory/memory-schema.ts 1-100](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/memory-schema.ts#L1-L100)

---

## Embedding Generation

### Provider Selection

```
openaigeminilocalautoYesNoYesNoYesNoresolveMemorySearchConfig()Provider?OpenAI Provider
text-embedding-3-smallGemini Provider
gemini-embedding-001Local Provider
node-llama-cppAuto ProviderCheck AvailableLocal model exists?OpenAI key available?Gemini key available?
```

**Embedding Provider Resolution**

The system resolves embedding providers based on configuration and available credentials:

1. If `provider` is explicitly set, use that provider
2. If `provider = "auto"`, check in order: local model path exists → OpenAI key available → Gemini key available
3. Fall back to disabled state if no provider can be resolved

Fallback providers can be configured via `memorySearch.fallback` to handle transient failures.

Sources: [src/memory/embeddings.ts 82-200](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/embeddings.ts#L82-L200) [src/agents/memory-search.ts 112-149](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/agents/memory-search.ts#L112-L149)

### Embedding Timeout Configuration

| Provider | Query Timeout | Batch Timeout | Source |
| --- | --- | --- | --- |
| Remote (OpenAI/Gemini) | 60 seconds | 120 seconds | [manager.ts 108-109](https://github.com/openclaw/openclaw/blob/bf6ec64f/manager.ts#L108-L109) |
| Local (llama.cpp) | 5 minutes | 10 minutes | [manager.ts 108-109](https://github.com/openclaw/openclaw/blob/bf6ec64f/manager.ts#L108-L109) |

**Timeout Values**

Timeouts are provider-specific to account for network latency (remote) vs compute time (local).

Sources: [src/memory/manager.ts 106-110](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L106-L110)

---

## Batch Processing

### OpenAI Batch API

```
SQLite CacheOpenAI Batch APIrunOpenAiEmbeddingBatchesMemoryIndexManagerSQLite CacheOpenAI Batch APIrunOpenAiEmbeddingBatchesMemoryIndexManagerloop[Poll every 2s]alt[wait=true][wait=false]loop[For each batch]chunks[], model, concurrencyGroup into batches(max 8000 tokens)POST /v1/batches(JSONL file)batch_idGET /v1/batches/{id}statuscompletedGET output fileembeddingsbatch_id (fire-and-forget)Cache embeddingsembeddings[]
```

**OpenAI Batch API Flow**

The OpenAI batch implementation:

1. Groups chunks into batches (respecting 8000 token limit per batch)
2. Creates JSONL batch files with custom IDs
3. Uploads batches to `/v1/batches` endpoint
4. Polls for completion (default every 2 seconds) if `wait=true`
5. Downloads output files and extracts embeddings
6. Caches results in SQLite `embedding_cache` table

Concurrent batch processing is controlled by `remote.batch.concurrency` (default: 2).

Sources: [src/memory/batch-openai.ts 1-350](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/batch-openai.ts#L1-L350) [src/memory/manager.ts 1013-1075](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L1013-L1075)

### Gemini Batch API

The Gemini batch implementation follows a similar pattern:

1. Groups chunks into batches
2. Creates batch requests via `batchEmbedContents` API
3. Polls operation status using `operations.get`
4. Extracts embeddings from completed batch response
5. Caches in embedding cache

Sources: [src/memory/batch-gemini.ts 1-280](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/batch-gemini.ts#L1-L280)

### Batch Configuration

| Setting | Default | Description |
| --- | --- | --- |
| `remote.batch.enabled` | `true` | Enable batch API |
| `remote.batch.wait` | `true` | Wait for completion |
| `remote.batch.concurrency` | `2` | Max concurrent batches |
| `remote.batch.pollIntervalMs` | `2000` | Poll delay (ms) |
| `remote.batch.timeoutMinutes` | `60` | Max wait time |

**Batch Settings**

Batch processing is enabled by default for OpenAI and Gemini to reduce API costs and improve indexing speed. Setting `wait=false` allows fire-and-forget batch submission, though embeddings won't be available until the next sync cycle.

Sources: [src/agents/memory-search.ts 133-144](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/agents/memory-search.ts#L133-L144) [CHANGELOG.md 171-172](https://github.com/openclaw/openclaw/blob/bf6ec64f/CHANGELOG.md#L171-L172)

---

## Embedding Cache

### Cache Schema

```
embedding_cachestringtext_hashPKstringproviderstringmodelblobembeddingintcreated_at
```

**Embedding Cache Table**

The `embedding_cache` table stores embeddings keyed by:

- `text_hash`: SHA-256 hash of chunk text
- `provider`: Provider ID (e.g., "openai", "gemini")
- `model`: Model name (e.g., "text-embedding-3-small")

This enables reuse across reindexing cycles when chunks remain unchanged or when switching between configurations that use the same provider/model.

Sources: [src/memory/memory-schema.ts 60-80](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/memory-schema.ts#L60-L80) [src/memory/manager.ts 883-935](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L883-L935)

### Cache Lifecycle

Cache entries are:

1. **Created** during embedding generation before storage
2. **Retrieved** during indexing to skip redundant API calls
3. **Pruned** when `cache.maxEntries` is exceeded (eviction policy: oldest first)

The cache is provider+model specific, so changing embedding models will result in cache misses.

Sources: [src/memory/manager.ts 883-935](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L883-L935)

---

## Storage and Indexing

### Database Tables

```
containshas vectorhas FTS entrycachesfilesstringpathPKstringhashintmtimeMsintsizestringproviderstringmodelintindexed_atchunksstringidPKstringfile_pathFKintstart_lineintend_linestringtextstringhashstringproviderstringmodelstringsourceintindexed_atchunks_vecblobembeddingchunks_ftsstringtextembedding_cachestringtext_hashPKstringproviderstringmodelblobembeddingintcreated_at
```

**Memory Index Schema**

Five core tables comprise the index:

1. **files**: Tracks indexed files with hashes and metadata
2. **chunks**: Stores all text chunks with line ranges
3. **chunks\_vec**: Virtual table for vector similarity search (sqlite-vec)
4. **chunks\_fts**: Full-text search index (FTS5) for keyword queries
5. **embedding\_cache**: Caches embeddings to prevent regeneration

The `chunks.id` column uses format `{file_path}:{start_line}-{end_line}:{hash}` for stable identifiers.

Sources: [src/memory/memory-schema.ts 1-150](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/memory-schema.ts#L1-L150) [src/memory/manager.ts 45-96](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L45-L96)

### Vector Table (sqlite-vec)

The `chunks_vec` virtual table is created when sqlite-vec extension is available:

```
CREATE VIRTUAL TABLE chunks_vec USING vec0(
  embedding float[{dimensions}]
)
```

Vector dimensions are determined by the embedding model:

- OpenAI `text-embedding-3-small`: 1536 dimensions
- Gemini `gemini-embedding-001`: 768 dimensions
- Local models: varies by model

The vector table is populated via `INSERT INTO chunks_vec(rowid, embedding)` where `rowid` matches the chunk record.

Sources: [src/memory/memory-schema.ts 100-120](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/memory-schema.ts#L100-L120) [src/memory/sqlite-vec.ts 1-80](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/sqlite-vec.ts#L1-L80)

### FTS5 Table

The `chunks_fts` table enables BM25 keyword ranking:

```
CREATE VIRTUAL TABLE chunks_fts USING fts5(
  text,
  provider,
  model,
  file_path UNINDEXED,
  start_line UNINDEXED,
  end_line UNINDEXED,
  source UNINDEXED,
  content='',
  tokenize='porter unicode61'
)
```

This table is populated separately from vectors and supports full-text search with Porter stemming.

Sources: [src/memory/memory-schema.ts 80-100](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/memory-schema.ts#L80-L100)

---

## Incremental Sync

### Change Detection

```
ChangedSameMissinglistMemoryFiles()Load indexed files
from DBCompare hashesReindex changed filesSkip unchangedDelete removed files
```

**Incremental Sync Logic**

The sync process compares current file hashes with stored hashes to determine which files need reprocessing:

1. Scan workspace for memory files
2. Load indexed file records from `files` table
3. Compare hashes:
	- **Changed**: Delete old chunks, rechunk, regenerate embeddings
	- **Unchanged**: Skip (no work needed)
	- **Removed**: Delete file and chunk records

This minimizes redundant API calls and reduces indexing time for large workspaces.

Sources: [src/memory/sync-memory-files.ts 1-100](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/sync-memory-files.ts#L1-L100) [src/memory/manager.ts 682-770](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L682-L770)

### Watch Mode

**File Watcher Integration**

When `sync.watch=true`, the manager starts a `chokidar` watcher monitoring:

- `MEMORY.md` and `memory.md` in workspace root
- All `.md` files in `memory/` directory
- All configured extra paths

File changes trigger a debounced sync (default 1500ms) to batch multiple rapid edits.

Sources: [src/memory/manager.ts 547-608](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L547-L608) [src/agents/memory-search.ts 78](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/agents/memory-search.ts#L78-L78)

---

## Session Transcript Indexing

### Session Source Configuration

Session transcript indexing is controlled by two settings:

1. `memorySearch.sources`: Must include `"sessions"` in the array
2. `memorySearch.experimental.sessionMemory`: Must be `true`

When enabled, the system indexes `.jsonl` transcript files from `~/.openclaw/agents/{agentId}/sessions/`.

Sources: [src/agents/memory-search.ts 90-102](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/agents/memory-search.ts#L90-L102) [src/memory/manager.ts 143](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L143-L143)

### Delta-Based Reindexing

```
YesNoonSessionTranscriptUpdate
eventTrack file in
sessionsDirtyFilesDebounce timer
(5000ms)Read delta bytes
since last indexExceeds
threshold?Reindex transcriptSkip
```

**Session Delta Tracking**

Session transcripts are large and grow continuously. To avoid reindexing entire transcripts on every change, the system tracks:

- `deltaBytes`: Bytes appended since last index
- `deltaMessages`: JSONL lines appended since last index

Thresholds:

- `deltaBytes >= 100,000` (100 KB)
- `deltaMessages >= 50`

Only new content beyond the last indexed position is processed.

Sources: [src/memory/manager.ts 617-680](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L617-L680) [src/agents/memory-search.ts 79-80](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/agents/memory-search.ts#L79-L80)

---

## CLI Operations

### Status Command

```
openclaw memory status [--agent <id>] [--deep] [--index] [--verbose]
```

Output includes:

- Workspace directory
- Memory sources (memory/sessions)
- File counts per source
- Index statistics (files indexed, chunks, dirty state)
- Provider/model information
- Vector availability (if `--deep`)
- FTS availability
- Embedding cache stats

The `--deep` flag probes vector extension loading. The `--index` flag forces a reindex if the store is dirty.

Sources: [src/cli/memory-cli.ts 115-350](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/cli/memory-cli.ts#L115-L350) [docs/cli/memory.md 1-43](https://github.com/openclaw/openclaw/blob/bf6ec64f/docs/cli/memory.md#L1-L43)

### Index Command

```
openclaw memory index [--agent <id>] [--verbose]
```

Forces a full reindex of all sources. With `--verbose`, displays:

- Provider and model details
- Source scan results
- Batch processing activity (when applicable)
- Progress updates
- Final statistics

Sources: [src/cli/memory-cli.ts 352-450](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/cli/memory-cli.ts#L352-L450)

### Search Command

```
openclaw memory search "query text" [--agent <id>] [--max-results <n>] [--min-score <n>]
```

Performs a semantic search and displays:

- Matching file paths
- Line ranges
- Relevance scores
- Snippet previews (up to 700 characters)
- Source type (memory/sessions)

Sources: [src/cli/memory-cli.ts 452-550](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/cli/memory-cli.ts#L452-L550) [docs/cli/memory.md 26-27](https://github.com/openclaw/openclaw/blob/bf6ec64f/docs/cli/memory.md#L26-L27)

---

## Progress Reporting

### Sync Progress Updates

The `sync()` method accepts a `progress` callback:

```
type MemorySyncProgressUpdate = {
  completed: number;
  total: number;
  label?: string;
};
```

Progress is reported during:

1. File discovery
2. Change detection
3. Batch embedding generation (per-batch updates)
4. Vector insertion
5. FTS population

This enables UI progress bars and verbose CLI output.

Sources: [src/memory/manager.ts 83-90](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L83-L90) [src/cli/memory-cli.ts 11-20](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/cli/memory-cli.ts#L11-L20)

---

### Batch Failure Tracking

The manager tracks batch failures to prevent infinite retry loops:

| Field | Purpose |
| --- | --- |
| `batchFailureCount` | Number of consecutive batch failures |
| `batchFailureLastError` | Error message from last failure |
| `batchFailureLastProvider` | Provider that failed |
| `batchFailureLock` | Promise to serialize failure handling |

When `batchFailureCount >= BATCH_FAILURE_LIMIT` (2), the system:

1. Falls back to non-batch embedding generation
2. Logs a warning with the failure reason
3. Continues indexing with individual API calls

Sources: [src/memory/manager.ts 139-142](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L139-L142) [src/memory/manager.ts 104](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L104-L104)

### Timeout Handling

Each embedding operation has configurable timeouts:

- Query embeddings (single chunk): 60s remote, 5min local
- Batch operations: 120s remote, 10min local

Timeouts trigger fallback to the next provider or disable memory search if no fallbacks are configured.

Sources: [src/memory/manager.ts 106-110](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L106-L110)

---

## Performance Optimizations

### Concurrency Controls

| Operation | Concurrency | Configuration |
| --- | --- | --- |
| File indexing | 4 | `EMBEDDING_INDEX_CONCURRENCY` |
| Batch jobs | 2 | `remote.batch.concurrency` |
| File I/O | Async | Native Node.js |

**Concurrency Limits**

The system uses `runWithConcurrency()` to parallelize embedding generation within safe limits. This prevents overwhelming embedding APIs or exhausting local compute resources.

Sources: [src/memory/manager.ts 100-101](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L100-L101) [src/agents/memory-search.ts 138](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/agents/memory-search.ts#L138-L138)

### Embedding Cache Efficiency

The cache prevents redundant embeddings by:

1. Computing SHA-256 hash of chunk text
2. Looking up `embedding_cache` by hash+provider+model
3. Returning cached embedding if found
4. Otherwise generating and caching

Cache hit rates improve significantly on subsequent reindexing when only a few files change.

Sources: [src/memory/manager.ts 883-935](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L883-L935)

---

## Extension Points

### Plugin Integration

Memory search is provided via the plugin slot system. The default plugin is `@openclaw/memory-core`:

```
{
  plugins: {
    slots: {
      memory: "memory-core"  // or "none" to disable
    }
  }
}
```

Custom memory plugins can implement alternative indexing strategies (e.g., using LanceDB or other vector databases).

Sources: [extensions/memory-core/package.json 1-17](https://github.com/openclaw/openclaw/blob/bf6ec64f/extensions/memory-core/package.json#L1-L17) [docs/concepts/memory.md 11-13](https://github.com/openclaw/openclaw/blob/bf6ec64f/docs/concepts/memory.md#L11-L13)

### Custom Providers

New embedding providers can be added by implementing the `EmbeddingProvider` interface:

```
type EmbeddingProvider = {
  id: string;
  model: string;
  embedQuery(text: string): Promise<number[]>;
  embedBatch(texts: string[]): Promise<number[][]>;
};
```

Sources: [src/memory/embeddings.ts 22-27](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/embeddings.ts#L22-L27)

---

## Summary

Memory indexing in OpenClaw transforms Markdown files into a searchable semantic index through:

1. **File Discovery**: Scanning workspace and configured paths for `.md` files
2. **Chunking**: Splitting files into token-sized overlapping segments
3. **Embedding**: Generating vectors via OpenAI, Gemini, or local models
4. **Batch Processing**: Using batch APIs for cost-effective bulk embedding
5. **Storage**: Persisting in SQLite with vector and FTS indexes
6. **Caching**: Storing embeddings to prevent redundant generation
7. **Incremental Sync**: Tracking file hashes to minimize reindexing
8. **Watch Mode**: Monitoring file changes for automatic updates

The result is a fast, cost-efficient semantic memory system that enables natural language queries over workspace knowledge.

Sources: [src/memory/manager.ts 1-1200](https://github.com/openclaw/openclaw/blob/bf6ec64f/src/memory/manager.ts#L1-L1200) [docs/concepts/memory.md 1-174](https://github.com/openclaw/openclaw/blob/bf6ec64f/docs/concepts/memory.md#L1-L174)

<svg id="mermaid-fu8dstus3vp" width="100%" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 2412 512" style="max-width: 512px;" role="graphics-document document" aria-roledescription="error"><g></g><g><path class="error-icon" d="m411.313,123.313c6.25-6.25 6.25-16.375 0-22.625s-16.375-6.25-22.625,0l-32,32-9.375,9.375-20.688-20.688c-12.484-12.5-32.766-12.5-45.25,0l-16,16c-1.261,1.261-2.304,2.648-3.31,4.051-21.739-8.561-45.324-13.426-70.065-13.426-105.867,0-192,86.133-192,192s86.133,192 192,192 192-86.133 192-192c0-24.741-4.864-48.327-13.426-70.065 1.402-1.007 2.79-2.049 4.051-3.31l16-16c12.5-12.492 12.5-32.758 0-45.25l-20.688-20.688 9.375-9.375 32.001-31.999zm-219.313,100.687c-52.938,0-96,43.063-96,96 0,8.836-7.164,16-16,16s-16-7.164-16-16c0-70.578 57.422-128 128-128 8.836,0 16,7.164 16,16s-7.164,16-16,16z"></path><path class="error-icon" d="m459.02,148.98c-6.25-6.25-16.375-6.25-22.625,0s-6.25,16.375 0,22.625l16,16c3.125,3.125 7.219,4.688 11.313,4.688 4.094,0 8.188-1.563 11.313-4.688 6.25-6.25 6.25-16.375 0-22.625l-16.001-16z"></path><path class="error-icon" d="m340.395,75.605c3.125,3.125 7.219,4.688 11.313,4.688 4.094,0 8.188-1.563 11.313-4.688 6.25-6.25 6.25-16.375 0-22.625l-16-16c-6.25-6.25-16.375-6.25-22.625,0s-6.25,16.375 0,22.625l15.999,16z"></path><path class="error-icon" d="m400,64c8.844,0 16-7.164 16-16v-32c0-8.836-7.156-16-16-16-8.844,0-16,7.164-16,16v32c0,8.836 7.156,16 16,16z"></path><path class="error-icon" d="m496,96.586h-32c-8.844,0-16,7.164-16,16 0,8.836 7.156,16 16,16h32c8.844,0 16-7.164 16-16 0-8.836-7.156-16-16-16z"></path><path class="error-icon" d="m436.98,75.605c3.125,3.125 7.219,4.688 11.313,4.688 4.094,0 8.188-1.563 11.313-4.688l32-32c6.25-6.25 6.25-16.375 0-22.625s-16.375-6.25-22.625,0l-32,32c-6.251,6.25-6.251,16.375-0.001,22.625z"></path><text class="error-text" x="1440" y="250" font-size="150px" style="text-anchor: middle;">Syntax error in text</text> <text class="error-text" x="1250" y="400" font-size="100px" style="text-anchor: middle;">mermaid version 11.12.2</text></g></svg> <svg id="mermaid-7khuxtjp8f" width="100%" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 2412 512" style="max-width: 512px;" role="graphics-document document" aria-roledescription="error"><g></g><g><path class="error-icon" d="m411.313,123.313c6.25-6.25 6.25-16.375 0-22.625s-16.375-6.25-22.625,0l-32,32-9.375,9.375-20.688-20.688c-12.484-12.5-32.766-12.5-45.25,0l-16,16c-1.261,1.261-2.304,2.648-3.31,4.051-21.739-8.561-45.324-13.426-70.065-13.426-105.867,0-192,86.133-192,192s86.133,192 192,192 192-86.133 192-192c0-24.741-4.864-48.327-13.426-70.065 1.402-1.007 2.79-2.049 4.051-3.31l16-16c12.5-12.492 12.5-32.758 0-45.25l-20.688-20.688 9.375-9.375 32.001-31.999zm-219.313,100.687c-52.938,0-96,43.063-96,96 0,8.836-7.164,16-16,16s-16-7.164-16-16c0-70.578 57.422-128 128-128 8.836,0 16,7.164 16,16s-7.164,16-16,16z"></path><path class="error-icon" d="m459.02,148.98c-6.25-6.25-16.375-6.25-22.625,0s-6.25,16.375 0,22.625l16,16c3.125,3.125 7.219,4.688 11.313,4.688 4.094,0 8.188-1.563 11.313-4.688 6.25-6.25 6.25-16.375 0-22.625l-16.001-16z"></path><path class="error-icon" d="m340.395,75.605c3.125,3.125 7.219,4.688 11.313,4.688 4.094,0 8.188-1.563 11.313-4.688 6.25-6.25 6.25-16.375 0-22.625l-16-16c-6.25-6.25-16.375-6.25-22.625,0s-6.25,16.375 0,22.625l15.999,16z"></path><path class="error-icon" d="m400,64c8.844,0 16-7.164 16-16v-32c0-8.836-7.156-16-16-16-8.844,0-16,7.164-16,16v32c0,8.836 7.156,16 16,16z"></path><path class="error-icon" d="m496,96.586h-32c-8.844,0-16,7.164-16,16 0,8.836 7.156,16 16,16h32c8.844,0 16-7.164 16-16 0-8.836-7.156-16-16-16z"></path><path class="error-icon" d="m436.98,75.605c3.125,3.125 7.219,4.688 11.313,4.688 4.094,0 8.188-1.563 11.313-4.688l32-32c6.25-6.25 6.25-16.375 0-22.625s-16.375-6.25-22.625,0l-32,32c-6.251,6.25-6.251,16.375-0.001,22.625z"></path><text class="error-text" x="1440" y="250" font-size="150px" style="text-anchor: middle;">Syntax error in text</text> <text class="error-text" x="1250" y="400" font-size="100px" style="text-anchor: middle;">mermaid version 11.12.2</text></g></svg>