{
  "slug": "agent-audit-and-replay",
  "title": "Agent Audit and Replay (Evidence, Retention, Rollback) | Claw EA",
  "category": "pillars",
  "html": "<h2>Direct Answer</h2>\n<p>Agent audit and replay is the discipline of making every agent run provable, reviewable, and reversible, even after the chat transcript is gone or disputed. In Claw EA, the operational unit is a run governed by a WPC, authenticated by a CST, and evidenced by gateway receipts and a proof bundle.</p>\n<p>OpenClaw is the baseline agent runtime, but prompt-only control is not enough because prompts are not permission boundaries. You need a permissioned execution layer with policy-as-code so tools, model calls, and replay constraints are enforced by machines, not “best effort” instructions.</p>\n\n<h2>Step-by-step runbook</h2>\n<p>This runbook aims for audit-ready operation without inventing new infrastructure. It assumes OpenClaw is your runtime, and Claw Bureau services are used for policy and evidence.</p>\n<ol>\n  <li>\n    <p>Define the work boundary for a run: objective, data classes touched, and allowed tools. Write it as a WPC and treat it like a change-controlled artifact that you can hash, sign, and fetch deterministically.</p>\n    <p>Keep the WPC stable for the entire run so later audits can answer “what was the agent allowed to do at that time?”</p>\n  </li>\n  <li>\n    <p>Issue a CST for the job and pin it to the policy hash when you need strict replay controls. The CST is the on-wire authorization, and the scope hash is the concise representation that auditors can compare across runs.</p>\n    <p>For anti-replay, use job-scoped CST binding so a token captured from one job cannot be reused to launder a different job’s activity.</p>\n  </li>\n  <li>\n    <p>Route model calls through clawproxy so you get gateway receipts for each model call. This is the evidence layer for “what model was called, with what request envelope, and what came back,” in a form that can be verified later.</p>\n    <p>If you use OpenRouter via fal, keep it on that path through clawproxy so receipts cover the routed call.</p>\n  </li>\n  <li>\n    <p>Lock down tool execution locally in OpenClaw using sandboxing and tool policy, and confirm the effective configuration before enabling external triggers. Prompt text can request anything, but OpenClaw tool allow/deny and sandbox settings decide what can actually execute.</p>\n    <p>Run periodic checks with OpenClaw security audit guidance, especially after changing network exposure, plugins, or channel policies.</p>\n  </li>\n  <li>\n    <p>At the end of the run, emit a proof bundle that includes gateway receipts and run metadata needed for later verification. Store the proof bundle with your retention policy, and optionally publish a Trust Pulse for audit viewing.</p>\n    <p>Retention should cover at least the period where decisions can be challenged (for example: access reviews, incident response, financial close).</p>\n  </li>\n  <li>\n    <p>Verify on demand: when an incident occurs, you want to verify evidence without re-running the agent. Treat verification as a fail-closed gate in your incident workflow: if a proof bundle cannot be verified, the run is not admissible as evidence.</p>\n    <p>Replay should be “replay the evidence,” not “replay the model.” You can re-simulate tool steps in a safe sandbox, but you should not claim deterministic reproduction of frontier model outputs.</p>\n  </li>\n</ol>\n\n<h2>Threat model</h2>\n<p>Audit and replay fail most often from small operational gaps: missing policy pinning, permissive tool profiles, or logs that cannot be tied to a specific job. The table below lists concrete failure modes and what to do about them.</p>\n<table>\n  <thead>\n    <tr>\n      <th>Threat</th>\n      <th>What happens</th>\n      <th>Control</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Prompt injection triggers unintended tool use</td>\n      <td>The agent follows hostile instructions embedded in content and calls file, shell, browser, or network tools.</td>\n      <td>Permissioned execution: enforce tool allow/deny in OpenClaw, use sandboxing for tool execution, and bind the run to a WPC that restricts tool categories.</td>\n    </tr>\n    <tr>\n      <td>Policy drift between runs</td>\n      <td>A team changes config and later cannot prove what was in effect when a decision was made.</td>\n      <td>Use a WPC as the hash-addressed policy artifact and pin the CST to the policy hash so the run carries its policy identity.</td>\n    </tr>\n    <tr>\n      <td>Token replay across jobs</td>\n      <td>A captured token is reused to make calls that appear to belong to a different run.</td>\n      <td>Marketplace anti-replay binding with job-scoped CST binding, plus storing the job identifier inside the proof bundle metadata.</td>\n    </tr>\n    <tr>\n      <td>Evidence without verifiability</td>\n      <td>You have transcripts or app logs, but cannot prove which model calls occurred or whether content was altered.</td>\n      <td>Route model calls via clawproxy to produce gateway receipts, and package them into a proof bundle for verification.</td>\n    </tr>\n    <tr>\n      <td>Over-collection of sensitive data in logs</td>\n      <td>Logs become a second breach surface, or retention violates internal policy.</td>\n      <td>Keep retention scoped to what you can justify; store proof bundles with access controls; minimize raw prompt and tool output retention where feasible while keeping receipts and policy hashes.</td>\n    </tr>\n    <tr>\n      <td>Unsafe local execution surface</td>\n      <td>An agent with host execution can modify system state, credentials, or other workloads.</td>\n      <td>Prefer OpenClaw sandboxing modes, avoid elevated execution unless explicitly required, and regularly run the OpenClaw security audit checks.</td>\n    </tr>\n  </tbody>\n</table>\n\n<h2>Policy-as-code example</h2>\n<p>Prompt-only “rules” are easy to bypass because they are interpreted by the model. A WPC is policy-as-code: it is a signed, hash-addressed policy artifact served by clawcontrols, so enforcement can check the hash and fail closed.</p>\n<p>Below is a compact, JSON-like sketch of what teams typically pin for audit and replay. Treat it as an example shape, not a guaranteed schema.</p>\n<pre>\n{\n  \"wpc_ref\": \"wpc:sha256:&lt;policy_hash&gt;\",\n  \"job\": {\n    \"job_id\": \"job_2026_02_11_001\",\n    \"owner\": \"team-finops\",\n    \"purpose\": \"invoice_reconciliation\"\n  },\n  \"auth\": {\n    \"cst_scope_hash\": \"&lt;scope_hash&gt;\",\n    \"pin_policy_hash\": true,\n    \"anti_replay\": \"job_scoped_binding\"\n  },\n  \"runtime\": {\n    \"baseline\": \"OpenClaw\",\n    \"sandbox\": { \"mode\": \"all\", \"workspaceAccess\": \"ro\" },\n    \"tools\": {\n      \"allow\": [\"read\", \"write\", \"http\", \"browser\"],\n      \"deny\": [\"exec\", \"elevated\"]\n    }\n  },\n  \"models\": {\n    \"route\": \"OpenRouter via fal (through clawproxy)\",\n    \"require_gateway_receipts\": true\n  },\n  \"retention\": {\n    \"proof_bundle_days\": 180,\n    \"access\": \"security-review-only\"\n  }\n}\n</pre>\n\n<h2>What proof do you get?</h2>\n<p>For audit, you need evidence that is machine-verifiable and tied to a specific job. In Claw EA, the core artifacts are gateway receipts and the proof bundle that packages them with run identifiers and policy references.</p>\n<p>Gateway receipts are signed receipts emitted by clawproxy for model calls. They help you answer: which model route was used, which request envelope was sent, what response was returned, and whether the call was authorized under the CST presented.</p>\n<p>A proof bundle is the harness artifact bundling receipts and related metadata for audit and verification. In practice, you store the proof bundle as the authoritative record of the run’s model interaction evidence, alongside the WPC reference and the CST scope hash used at execution time.</p>\n<p>For operational review, you can also publish a Trust Pulse as a marketplace-stored artifact for audit/viewing. Use it when you want a stable, shareable view of the run evidence without handing out raw internal logs by default.</p>\n<p>Retention is a policy decision, not a logging accident. Keep the proof bundle long enough to support incident response and contested decisions, and keep access tight so evidence does not become an unbounded data lake.</p>\n\n<h2>Rollback posture</h2>\n<p>Rollback in agent systems is mostly about containing blast radius and restoring known-good state. You rarely “roll back the model,” but you can roll back credentials, permissions, channel exposure, and the tool surface while preserving evidence.</p>\n<table>\n  <thead>\n    <tr>\n      <th>Action</th>\n      <th>Safe rollback</th>\n      <th>Evidence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Suspected compromised job token</td>\n      <td>Revoke the CST and re-issue a new CST for a new job. Keep job-scoped CST binding so old tokens cannot be repurposed.</td>\n      <td>Proof bundle shows the CST scope hash used per run and the gateway receipts show calls made under that authorization.</td>\n    </tr>\n    <tr>\n      <td>Policy mistake (too-permissive tools)</td>\n      <td>Publish a new WPC with tightened permissions, then pin the new policy hash for subsequent runs. Do not “edit” the old WPC for historical runs.</td>\n      <td>WPC hash identifies the policy in effect; proof bundle and receipts tie activity to that specific policy identity.</td>\n    </tr>\n    <tr>\n      <td>Unexpected model behavior in production</td>\n      <td>Freeze the model route for the workflow and require future calls to route through clawproxy for receipting. If needed, disable high-risk tools in OpenClaw tool policy immediately.</td>\n      <td>Gateway receipts provide the model-call evidence for the impacted window; OpenClaw configuration and audit guidance support post-change review.</td>\n    </tr>\n    <tr>\n      <td>Channel exposure or inbound abuse</td>\n      <td>Tighten inbound allowlists and mention requirements in OpenClaw channel policy. Reduce the number of places where strangers can trigger runs.</td>\n      <td>OpenClaw security audit guidance and configuration state support the “what changed” narrative; proof bundles cover model-call evidence for triggered runs.</td>\n    </tr>\n    <tr>\n      <td>Need to prove what happened without rerunning</td>\n      <td>Do not rerun the job as a first step. Verify the proof bundle and review receipts and run metadata; use sandboxed re-simulation only for contained reproduction.</td>\n      <td>Proof bundle is the authoritative evidence package; receipts are signed and can be checked for integrity.</td>\n    </tr>\n  </tbody>\n</table>\n\n<h2>FAQ</h2>\n\n<h3>Why isn’t a transcript enough for audit and replay?</h3>\n<p>A transcript is easy to truncate, redact incorrectly, or reformat without detection. Gateway receipts and a proof bundle are designed to be verified, and they bind model-call evidence to a job, a CST scope hash, and a WPC reference.</p>\n\n<h3>What does “replay” mean if model outputs are nondeterministic?</h3>\n<p>Replay means replaying the evidence and the execution constraints, not guaranteeing identical text output. You can validate that specific model calls occurred and then re-simulate tool steps safely, but you should not treat a second model run as the same event.</p>\n\n<h3>How does policy-as-code stop prompt injection better than prompt rules?</h3>\n<p>Prompt rules are suggestions to the model, and the model can be coerced or can make mistakes. Policy-as-code is enforced outside the model: OpenClaw controls which tools can run, and WPC plus CST pinning makes the allowed surface explicit and verifiable.</p>\n\n<h3>What is the minimum set of artifacts to retain for an audit-ready posture?</h3>\n<p>At minimum: the WPC reference (hash), the CST scope hash used for the job, and the proof bundle containing gateway receipts. Keep additional application logs only if they are necessary, and keep access to evidence limited to review roles.</p>\n\n<h3>How does this relate to Microsoft audit logs?</h3>\n<p>If you operate in Microsoft environments, keep your agent governance aligned with Microsoft audit logging and review flows for agent activity. Use Microsoft terminology and controls where applicable (for example: Entra ID, Conditional Access, PIM), and integrate via official API where needed rather than assuming native connectors.</p>\n\n<h2>Sources</h2>\n<ul>\n  <li><a href=\"https://github.com/openclaw/openclaw/blob/5d4f42016f3afdbd5218843648d3ea594541dedc/docs/gateway/security/index.md\">OpenClaw Gateway Security (audit + footguns)</a></li>\n  <li><a href=\"https://github.com/openclaw/openclaw/blob/5d4f42016f3afdbd5218843648d3ea594541dedc/docs/gateway/sandbox-vs-tool-policy-vs-elevated.md\">OpenClaw: Sandbox vs Tool Policy vs Elevated</a></li>\n  <li><a href=\"https://github.com/openclaw/openclaw/blob/5d4f42016f3afdbd5218843648d3ea594541dedc/docs/gateway/sandboxing.md\">OpenClaw: Sandboxing</a></li>\n  <li><a href=\"https://learn.microsoft.com/en-us/microsoft-copilot-studio/admin-logging-copilot-studio\">View audit logs for admins, makers, and users of Copilot Studio (Microsoft Learn)</a></li>\n  <li><a href=\"https://learn.microsoft.com/en-us/microsoft-copilot-studio/authoring-review-activity\">Review agent activity (Microsoft Learn)</a></li>\n</ul>",
  "description": "Agent audit and replay is the discipline of making every agent run provable, reviewable, and reversible, even after the chat transcript is gone or disputed. In Claw EA, the operational unit is a run governed by a WPC, au",
  "faqs": [
    {
      "q": "Why isn’t a transcript enough for audit and replay?",
      "a": "A transcript is easy to truncate, redact incorrectly, or reformat without detection. Gateway receipts and a proof bundle are designed to be verified, and they bind model-call evidence to a job, a CST scope hash, and a WPC reference."
    },
    {
      "q": "What does “replay” mean if model outputs are nondeterministic?",
      "a": "Replay means replaying the evidence and the execution constraints, not guaranteeing identical text output. You can validate that specific model calls occurred and then re-simulate tool steps safely, but you should not treat a second model run as the same event."
    },
    {
      "q": "How does policy-as-code stop prompt injection better than prompt rules?",
      "a": "Prompt rules are suggestions to the model, and the model can be coerced or can make mistakes. Policy-as-code is enforced outside the model: OpenClaw controls which tools can run, and WPC plus CST pinning makes the allowed surface explicit and verifiable."
    },
    {
      "q": "What is the minimum set of artifacts to retain for an audit-ready posture?",
      "a": "At minimum: the WPC reference (hash), the CST scope hash used for the job, and the proof bundle containing gateway receipts. Keep additional application logs only if they are necessary, and keep access to evidence limited to review roles."
    },
    {
      "q": "How does this relate to Microsoft audit logs?",
      "a": "If you operate in Microsoft environments, keep your agent governance aligned with Microsoft audit logging and review flows for agent activity. Use Microsoft terminology and controls where applicable (for example: Entra ID, Conditional Access, PIM), and integrate via official API where needed rather than assuming native connectors."
    }
  ],
  "sources": [
    {
      "title": "View audit logs for admins, makers, and users of Copilot Studio - Microsoft Copilot Studio",
      "uri": "https://learn.microsoft.com/en-us/microsoft-copilot-studio/admin-logging-copilot-studio"
    },
    {
      "title": "Review agent activity - Microsoft Copilot Studio",
      "uri": "https://learn.microsoft.com/en-us/microsoft-copilot-studio/authoring-review-activity"
    },
    {
      "title": "How Agentic Memory Enables Reliable AI Agents Across Enterprise ...",
      "uri": "https://engineering.salesforce.com/how-agentic-memory-enables-durable-reliable-ai-agents-across-millions-of-enterprise-users/"
    }
  ],
  "model": "candidate-01",
  "generatedAt": "2026-02-11T10:41:33.539Z",
  "indexable": true
}