{
  "slug": "workflows/production-deploy-approval",
  "title": "Production deploy with two-person approval | Secure Agent Workflow | Claw EA",
  "category": "workflows",
  "html": "<h2>Direct Answer</h2>\n<p>To run production deploys with two-person approval, treat “deploy” as a permissioned action gated by policy-as-code, not a prompt instruction. In Claw EA, you do this by binding an agent run to a WPC and issuing a CST that is pinned to that policy hash, so the execution layer can fail-closed when approvals or dry-run evidence are missing.</p>\n<p>OpenClaw is the baseline agent runtime, but prompts alone cannot guarantee that a deploy only happens after two humans approve. The enforcement must live in the execution path, so tool calls and model calls are only accepted when the WPC conditions are satisfied and verifiable artifacts are attached.</p>\n\n<h2>Step-by-step runbook</h2>\n<p>Use this runbook when an agent proposes a production change and you want machine-enforced approvals, forced dry-run, and audit-ready evidence. The details of the approval UI and ticketing can be implemented via official API or via MCP server, but the enforcement points stay the same.</p>\n\n<ol>\n  <li>\n    <p><strong>Define the production deploy WPC.</strong> Put “deploy-to-prod” behind an explicit policy gate that requires two distinct approvers and a dry-run artifact. Store the WPC in the WPC registry so it is signed and hash-addressed (served by clawcontrols).</p>\n  </li>\n  <li>\n    <p><strong>Issue a CST that pins the policy hash.</strong> When a deploy job is created, mint a CST (issued by clawscope) scoped to only the deploy tool and the target environment, optionally pinning the WPC hash. This makes the job non-replayable across unrelated runs and gives you a stable reference for verification.</p>\n  </li>\n  <li>\n    <p><strong>Run “plan/dry-run” first and capture outputs.</strong> The agent must execute a forced dry-run before any production apply. If you use Terraform, this is “plan”; if you use Kubernetes, this might be “server-side dry run” plus a diff; if you use a custom deployer, produce a deterministic preview artifact.</p>\n  </li>\n  <li>\n    <p><strong>Collect two-person approval as signed inputs.</strong> Two humans approve the exact dry-run output, not a paraphrase. The approvals can come from your control plane (enterprise buildout), or from an existing workflow system via official API, but each approval must record approver identity, timestamp, and the hash of the dry-run artifact.</p>\n  </li>\n  <li>\n    <p><strong>Execute “apply” only after approvals validate.</strong> The deploy tool validates the WPC requirements locally before running any irreversible action. If approvals are missing, mismatched to the dry-run hash, or from the same identity, the tool fails closed and produces a denial record.</p>\n  </li>\n  <li>\n    <p><strong>Proxy model calls through clawproxy.</strong> Route the agent’s model calls through clawproxy so you get Gateway receipts for model calls. This lets you later prove which prompts and outputs were used during the decision path without relying on best-effort logging.</p>\n  </li>\n  <li>\n    <p><strong>Bundle evidence for audit.</strong> At the end of the job, assemble a proof bundle that includes the Gateway receipts, the WPC hash, the CST scope hash, dry-run artifacts, and approval records. Optionally publish the resulting artifact into Trust Pulse for viewing.</p>\n  </li>\n</ol>\n\n<h2>Threat model</h2>\n<p>Two-person approval exists because production deploys are irreversible or expensive to unwind. The common failure mode in agent systems is that a prompt suggests a constraint, but the tool still executes because nothing in the runtime enforces it.</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Threat</th>\n      <th>What happens</th>\n      <th>Control</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Prompt injection pushes a direct “deploy now” instruction</td>\n      <td>The agent attempts to skip approvals and calls the deploy tool directly</td>\n      <td>Policy-as-code WPC gating on the deploy tool, enforced at execution time; missing approvals cause fail-closed</td>\n    </tr>\n    <tr>\n      <td>Single approver rubber-stamps twice</td>\n      <td>Two approvals exist but are not independent</td>\n      <td>Two-person rule requires distinct identities; enforce “unique approver” and reject duplicates</td>\n    </tr>\n    <tr>\n      <td>Approval does not match what is deployed</td>\n      <td>Humans approve a plan, but the agent applies a different change</td>\n      <td>Forced dry-run; approvals bind to dry-run artifact hash; deploy tool verifies hash match before apply</td>\n    </tr>\n    <tr>\n      <td>Replay of a previously approved deploy</td>\n      <td>A past approval is reused for a new deploy attempt</td>\n      <td>Marketplace anti-replay binding using job-scoped CST binding; approvals must be job-bound and hash-bound</td>\n    </tr>\n    <tr>\n      <td>Unbounded tool blast radius in the runtime</td>\n      <td>The agent uses extra tools to exfiltrate secrets or change IAM as a side effect</td>\n      <td>OpenClaw tool policy and sandboxing reduce available tools and isolate execution; WPC restricts which actions are in-scope</td>\n    </tr>\n    <tr>\n      <td>Dispute about what the model saw or produced</td>\n      <td>Post-incident, you cannot prove whether a risky instruction came from the model or a human</td>\n      <td>Gateway receipts from clawproxy provide signed receipts for model calls; receipts are packaged into the proof bundle</td>\n    </tr>\n  </tbody>\n</table>\n\n<h2>Policy-as-code example</h2>\n<p>This is a compact, JSON-like sketch of a WPC that enforces three things: forced dry-run, two distinct approvals, and a strict tool scope for production apply. The enforcement should happen inside the deploy tool wrapper or the execution gateway that brokers the tool call.</p>\n\n<pre>\n{\n  \"wpc_version\": \"v1\",\n  \"policy_name\": \"prod_deploy_two_person_approval\",\n  \"risk_class\": \"irreversible\",\n  \"tool_scope\": {\n    \"allow\": [\n      \"deploy.dry_run\",\n      \"deploy.apply_prod\",\n      \"read.repo\",\n      \"read.build_artifacts\"\n    ],\n    \"deny\": [\n      \"rotate_credentials\",\n      \"change_firewall\",\n      \"iam.write\"\n    ]\n  },\n  \"required_sequence\": [\n    {\n      \"step\": \"dry_run\",\n      \"evidence\": {\n        \"artifact_type\": \"deploy_preview\",\n        \"hash_alg\": \"sha256\"\n      }\n    },\n    {\n      \"step\": \"approval\",\n      \"rule\": \"two_person\",\n      \"constraints\": {\n        \"distinct_identities\": true,\n        \"bind_to_artifact_hash\": \"dry_run.deploy_preview.sha256\"\n      }\n    },\n    {\n      \"step\": \"apply\",\n      \"guard\": {\n        \"must_match_artifact_hash\": \"dry_run.deploy_preview.sha256\"\n      }\n    }\n  ],\n  \"token_requirements\": {\n    \"cst\": {\n      \"scope_hash_required\": true,\n      \"optional_policy_hash_pinning\": true,\n      \"job_bound\": true\n    }\n  }\n}\n</pre>\n\n<p>Why policy-as-code instead of prompt-only: prompts can request approvals, but they cannot prevent a tool call from being made. A WPC is evaluated in the execution path, so “no approvals” is not a suggestion, it is a hard stop.</p>\n\n<h2>What proof do you get?</h2>\n<p>Each deploy job can produce evidence that is checkable after the fact, without trusting the agent’s narrative. The core artifacts are Gateway receipts, a proof bundle, and the policy and token bindings that show what was allowed.</p>\n\n<ul>\n  <li>\n    <p><strong>Gateway receipts.</strong> Signed receipts emitted by clawproxy for model calls, including the job context needed to verify that the model traffic was routed through the proxy. This is the backbone for answering “what did the model see and output?”</p>\n  </li>\n  <li>\n    <p><strong>WPC reference.</strong> The proof bundle includes the WPC hash (and optionally the full WPC) so reviewers can reconstruct the exact constraints in force for that job. Because WPCs are signed and hash-addressed, you can verify you are looking at the same policy that was enforced.</p>\n  </li>\n  <li>\n    <p><strong>CST scope hash and policy pinning.</strong> The CST (issued by clawscope) can be validated against its scope hash and, when used, the pinned WPC hash. This ties the job’s permissions to a specific contract and blocks “scope drift” during execution.</p>\n  </li>\n  <li>\n    <p><strong>Proof bundle.</strong> A harness artifact bundling receipts and related metadata for audit and verification, including dry-run artifact hashes and the two approval records. This is what you hand to security, change management, or auditors.</p>\n  </li>\n  <li>\n    <p><strong>Trust Pulse (optional).</strong> If you need a consistent place for reviewers to view an audit artifact, you can store the proof bundle as a Trust Pulse artifact for audit/viewing.</p>\n  </li>\n</ul>\n\n<h2>Rollback posture</h2>\n<p>Two-person approval reduces the chance of a bad deploy, but you still need an operational rollback posture. The key is to make rollback a first-class action with its own policy gates and its own evidence, rather than an ad hoc “fix forward” chat.</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>Action</th>\n      <th>Safe rollback</th>\n      <th>Evidence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Prod deploy apply</td>\n      <td>Prefer atomic deploy mechanisms (blue/green, canary, or versioned releases) so rollback is a version switch, not a manual edit</td>\n      <td>Proof bundle includes dry-run hash, two approvals bound to that hash, and Gateway receipts for model calls</td>\n    </tr>\n    <tr>\n      <td>Config change</td>\n      <td>Store prior configuration and use an automated revert path that re-applies the last known good version</td>\n      <td>WPC identifies the exact tool scope used; receipts show the sequence of calls leading to the change</td>\n    </tr>\n    <tr>\n      <td>Credential rotation</td>\n      <td>Use staged rotation with overlap and a documented break-glass path; treat rotation as high risk and gate it separately</td>\n      <td>Separate WPC and CST scope; proof bundle for rotation should be distinct from deploy proof bundles</td>\n    </tr>\n    <tr>\n      <td>IAM or firewall changes</td>\n      <td>Use least privilege and time-bound access; apply changes via infrastructure-as-code so rollback is a revert and apply</td>\n      <td>WPC denies these in the deploy workflow by default; any exception requires a different WPC and new approvals</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>OpenClaw-specific note: do not rely on “the agent will behave” when host execution is available. Use OpenClaw sandboxing and tool policy to limit what the agent can touch, and keep elevated host execution tightly controlled.</p>\n\n<h2>FAQ</h2>\n\n<h3>How is two-person approval enforced if the agent is the one running the deploy?</h3>\n<p>The deploy tool wrapper checks for two distinct approvals bound to the dry-run hash before executing apply. The WPC makes that requirement machine-evaluable, and the CST can be pinned to the WPC hash so the job cannot “switch policies” mid-run.</p>\n\n<h3>Why can’t we just put “wait for approval” in the prompt?</h3>\n<p>A prompt is not an enforcement boundary. If the runtime allows the tool call, the agent can still call it, especially under injection or accidental instructions, so the control must live in policy-as-code evaluated at execution time.</p>\n\n<h3>What counts as a “dry-run” in this workflow?</h3>\n<p>A dry-run is any deterministic preview output that can be hashed and later compared against the apply inputs. Examples include Terraform plan output, Kubernetes server-side dry run diffs, or a signed release manifest generated by your build system.</p>\n\n<h3>Can the approvals come from Microsoft tools?</h3>\n<p>Yes, via official API and your existing identity controls (for example Entra ID backed approvers), but Claw EA does not assume a native connector. The important part is that each approval record includes the approver identity and the dry-run artifact hash it is approving.</p>\n\n<h3>What do we show auditors after an incident?</h3>\n<p>Provide the proof bundle for the job: the WPC hash, the CST scope hash (and any policy pinning), the dry-run artifact hash, the two approval records, and Gateway receipts for model calls. This lets an auditor independently verify that the deploy was gated and that the model traffic was receipted.</p>\n\n<h2>Sources</h2>\n<ul>\n  <li><a href=\"https://github.com/openclaw/openclaw/blob/5d4f42016f3afdbd5218843648d3ea594541dedc/docs/gateway/security/index.md\">OpenClaw Gateway Security (audit + footguns)</a></li>\n  <li><a href=\"https://github.com/openclaw/openclaw/blob/5d4f42016f3afdbd5218843648d3ea594541dedc/docs/gateway/sandbox-vs-tool-policy-vs-elevated.md\">OpenClaw: Sandbox vs Tool Policy vs Elevated</a></li>\n  <li><a href=\"https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ai-agents/governance-security-across-organization\">Governance and security for AI agents across the organization (Microsoft Learn)</a></li>\n  <li><a href=\"https://learn.microsoft.com/en-us/microsoft-copilot-studio/flows-advanced-approvals\">Multistage and AI approvals in agent flows (Microsoft Learn)</a></li>\n</ul>\n\n<div class=\"cta-banner\" data-cta=\"bofu-endcap\">\n  <h2>Ready to put this workflow into production?</h2>\n  <p>Get a scoped deployment plan with Work Policy Contracts, approval gates, and cryptographic proof bundles for your team.</p>\n  <a href=\"/contact\" class=\"cta-btn cta-btn-lg\" data-cta=\"bofu-talk-to-sales\">Talk to Sales</a>\n  <a href=\"/trust\" class=\"cta-btn cta-btn-outline cta-btn-lg\" style=\"margin-left:.75rem\" data-cta=\"bofu-trust-layer\">Review Trust Layer</a>\n</div>\n",
  "description": "To run production deploys with two-person approval, treat “deploy” as a permissioned action gated by policy-as-code, not a prompt instruction. In Claw EA, you do this by binding an agent run to a WPC and issuing a CST th",
  "faqs": [
    {
      "q": "How is two-person approval enforced if the agent is the one running the deploy?",
      "a": "The deploy tool wrapper checks for two distinct approvals bound to the dry-run hash before executing apply. The WPC makes that requirement machine-evaluable, and the CST can be pinned to the WPC hash so the job cannot “switch policies” mid-run."
    },
    {
      "q": "Why can’t we just put “wait for approval” in the prompt?",
      "a": "A prompt is not an enforcement boundary. If the runtime allows the tool call, the agent can still call it, especially under injection or accidental instructions, so the control must live in policy-as-code evaluated at execution time."
    },
    {
      "q": "What counts as a “dry-run” in this workflow?",
      "a": "A dry-run is any deterministic preview output that can be hashed and later compared against the apply inputs. Examples include Terraform plan output, Kubernetes server-side dry run diffs, or a signed release manifest generated by your build system."
    },
    {
      "q": "Can the approvals come from Microsoft tools?",
      "a": "Yes, via official API and your existing identity controls (for example Entra ID backed approvers), but Claw EA does not assume a native connector. The important part is that each approval record includes the approver identity and the dry-run artifact hash it is approving."
    },
    {
      "q": "What do we show auditors after an incident?",
      "a": "Provide the proof bundle for the job: the WPC hash, the CST scope hash (and any policy pinning), the dry-run artifact hash, the two approval records, and Gateway receipts for model calls. This lets an auditor independently verify that the deploy was gated and that the model traffic was receipted."
    }
  ],
  "sources": [
    {
      "title": "Multistage and AI approvals in agent flows - Microsoft Copilot Studio",
      "uri": "https://learn.microsoft.com/en-us/microsoft-copilot-studio/flows-advanced-approvals"
    },
    {
      "title": "Governance and security for AI agents across the organization",
      "uri": "https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/ai-agents/governance-security-across-organization"
    },
    {
      "title": "Introducing Amazon Bedrock AgentCore: Securely deploy and ...",
      "uri": "https://aws.amazon.com/blogs/aws/introducing-amazon-bedrock-agentcore-securely-deploy-and-operate-ai-agents-at-any-scale/"
    },
    {
      "title": "AI agents in enterprises: Best practices with Amazon Bedrock ...",
      "uri": "https://aws.amazon.com/blogs/machine-learning/ai-agents-in-enterprises-best-practices-with-amazon-bedrock-agentcore/"
    },
    {
      "title": "Managing access for deployed agents",
      "uri": "https://cloud.google.com/agent-builder/agent-engine/manage/access"
    },
    {
      "title": "Best Practices for Secure Agentforce Implementation - Salesforce",
      "uri": "https://www.salesforce.com/blog/best-practices-for-secure-agentforce-implementation-2/"
    }
  ],
  "model": "candidate-01",
  "generatedAt": "2026-02-11T11:46:50.114Z",
  "indexable": true
}